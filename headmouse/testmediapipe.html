<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>MediaPipe Gaze â€“ dynamic ES-module imports</title>
  <style>
    html,body{margin:0;height:100%;background:#1a1a1a;overflow:hidden}
    #video{
      position:absolute;top:16px;left:16px;
      object-fit:cover;border:2px solid #fff;border-radius:14px;
      box-shadow:0 0 18px #000a;z-index:2;
    }
    #overlay{
      position:absolute;top:16px;left:16px;pointer-events:none;z-index:3;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="overlay"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.16.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection/dist/face-landmarks-detection.min.js"></script>
  <script>
    (async () => {
      /* ---------------------------------------------------------------
         2.  DOM setup
      ----------------------------------------------------------------*/
      const video  = document.getElementById('video');
      const canvas = /** @type {HTMLCanvasElement} */ (document.getElementById('overlay'));
      const ctx    = canvas.getContext('2d');

      /* ---------------------------------------------------------------
         3.  Webcam
      ----------------------------------------------------------------*/
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      await new Promise(r => video.onloadedmetadata = r);
      const {videoWidth:vw, videoHeight:vh} = video;
      video.width = canvas.width = vw;
      video.height = canvas.height = vh;
      video.style.width = canvas.style.width = vw + 'px';
      video.style.height = canvas.style.height = vh + 'px';

      /* ---------------------------------------------------------------
         4.  TFJS backend + model
      ----------------------------------------------------------------*/
      await tf.setBackend('webgl');
      await tf.ready();

      const model = await faceLandmarksDetection.createDetector(
        faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
        { runtime: 'tfjs', maxFaces: 1, refineLandmarks: true }
      );

      /* ---------------------------------------------------------------
         5.  Helpers
      ----------------------------------------------------------------*/
      const L=[468,469,470,471], R=[473,474,475,476];         // iris indices
      const mid = (a,b)=>[(a[0]+b[0])/2,(a[1]+b[1])/2];

      /* ---------------------------------------------------------------
         6.  Render loop
      ----------------------------------------------------------------*/
      async function draw(){
        const faces = await model.estimateFaces(video, { flipHorizontal: true });
        ctx.clearRect(0,0,canvas.width,canvas.height);

        if (faces.length){
          const mesh = faces[0].keypoints;
          for (const {x,y} of mesh) {
            ctx.beginPath();
            ctx.arc(x,y,1.5,0,Math.PI*2);
            ctx.fillStyle = '#0ff';
            ctx.fill();
          }

          const l = L.map(i=>[mesh[i].x, mesh[i].y]),
                r = R.map(i=>[mesh[i].x, mesh[i].y]);

          const lC = l.reduce((s,p)=>[s[0]+p[0],s[1]+p[1]],[0,0]).map(v=>v/l.length);
          const rC = r.reduce((s,p)=>[s[0]+p[0],s[1]+p[1]],[0,0]).map(v=>v/r.length);
          const gaze = mid(lC, rC);

          ctx.beginPath();
          ctx.arc(gaze[0], gaze[1], 24, 0, Math.PI*2);
          ctx.fillStyle = 'rgba(255,215,0,.65)';
          ctx.strokeStyle = '#fff';
          ctx.lineWidth = 3;
          ctx.fill();
          ctx.stroke();
        }
        requestAnimationFrame(draw);
      }
      draw();
    })();
  </script>
</body>
</html>
