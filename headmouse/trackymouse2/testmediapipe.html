<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>MediaPipe Gaze â€“ dynamic ES-module imports</title>
  <style>
    html,body{margin:0;height:100%;background:#1a1a1a;overflow:hidden}
    #video{
      position:absolute;top:16px;left:16px;width:220px;height:165px;
      object-fit:cover;border:2px solid #fff;border-radius:14px;
      box-shadow:0 0 18px #000a;z-index:2;
    }
    #overlay{position:fixed;inset:0;pointer-events:none}
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="overlay"></canvas>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.16.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@3.0.1/dist/face-landmarks-detection.min.js"></script>
  <script>
    (async () => {
      /* ---------------------------------------------------------------
         2.  DOM setup
      ----------------------------------------------------------------*/
      const video  = document.getElementById('video');
      const canvas = /** @type {HTMLCanvasElement} */ (document.getElementById('overlay'));
      const ctx    = canvas.getContext('2d');
      const resize = () => { canvas.width = innerWidth; canvas.height = innerHeight; };
      addEventListener('resize', resize); resize();

      /* ---------------------------------------------------------------
         3.  Webcam
      ----------------------------------------------------------------*/
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      await new Promise(r => video.onloadedmetadata = r);

      /* ---------------------------------------------------------------
         4.  TFJS backend + model
      ----------------------------------------------------------------*/
      await tf.setBackend('webgl');
      await tf.ready();

      const model = await faceLandmarksDetection.load(
        faceLandmarksDetection.SupportedPackages.mediapipeFacemesh,
        { maxFaces: 1, shouldLoadIrisModel: true }
      );

      /* ---------------------------------------------------------------
         5.  Helpers
      ----------------------------------------------------------------*/
      const L=[468,469,470,471], R=[473,474,475,476];         // iris indices
      const mid = (a,b)=>[(a[0]+b[0])/2,(a[1]+b[1])/2];

      /* ---------------------------------------------------------------
         6.  Render loop
      ----------------------------------------------------------------*/
      async function draw(){
        const faces = await model.estimateFaces({ input: video, flipHorizontal: true });
        ctx.clearRect(0,0,canvas.width,canvas.height);

        if (faces.length){
          const mesh = faces[0].scaledMesh;
          const l = L.map(i=>mesh[i]), r = R.map(i=>mesh[i]);

          const lC = l.reduce((s,p)=>[s[0]+p[0],s[1]+p[1]],[0,0]).map(v=>v/l.length);
          const rC = r.reduce((s,p)=>[s[0]+p[0],s[1]+p[1]],[0,0]).map(v=>v/r.length);
          const gaze = mid(lC, rC);

          ctx.beginPath();
          ctx.arc(gaze[0], gaze[1], 24, 0, Math.PI*2);
          ctx.fillStyle = 'rgba(255,215,0,.65)';
          ctx.strokeStyle = '#fff';
          ctx.lineWidth = 3;
          ctx.fill();
          ctx.stroke();
        }
        requestAnimationFrame(draw);
      }
      draw();
    })();
  </script>
</body>
</html>
